# -*- coding: utf-8 -*-
"""Predicting Wine Varietal Based on Chemical Properties.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TU8stPKWdV1Eleo41SoljXUHSWD6W6T0

# **Machine Learning and Pattern Recognition**

##Import libraries
"""

import numpy as np
import pandas as pd

"""##Import data set from the site and add the column names"""

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
column_names = ['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium',
                'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']
data = pd.read_csv(url, header=None, names=column_names)
print(data.head())

"""##See is there any null values in data set"""

data.isnull().sum()

"""##Create feature and target variables"""

X = data.drop('Class', axis=1)
y = data['Class']

"""##See is there have outliers from boxplot"""

data.boxplot(return_type='dict')
plt.plot()

"""##Calculate z-score for each features and remove outliers"""

z_scores = np.abs((X - X.mean(axis =0)) / X.std(axis=0))
threshold = 3
outliers = (z_scores > threshold)
print(outliers)

X_nooutliers_z = X[(~outliers).all(axis=1)]
y_nooutliers_z = y[(~outliers).all(axis=1)]
print("X shape after Z-score outliers removal: ", X_nooutliers_z.shape)
print("y shape after Z-score outliers removal: ", y_nooutliers_z.shape)

X_clean = X_nooutliers_z
y_clean = y_nooutliers_z

"""##Standardize the Features"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_clean)
print(X_scaled)

"""##Apply Principle component Analysis(PCA)"""

from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_scaled)

"""##Cross-Validation"""

from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

"""##Apply KNN model and other libraries"""

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

"""##Train data using cleaned data set"""

X_train, X_test, y_train, y_test = train_test_split(X_pca, y_clean, test_size=0.3, random_state=42)

k_values = range(1, 21)
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    cv_scores = cross_val_score(knn, X_pca, y_clean, cv=10, scoring='accuracy')
    cv_accuracy = np.mean(cv_scores)
    accuracies.append(cv_accuracy)
    print(f"k={k}, Accuracy: {cv_accuracy:.4f}")

"""##Make a figure using accuracy we got"""

plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracies, marker='o')
plt.xlabel('k (Number of Neighbors)')
plt.ylabel('Accuracy')
plt.title('k-NN Classification Accuracy for Different k Values')
plt.grid(True)
plt.show()

best_k = k_values[np.argmax(accuracies)]
best_accuracy = max(accuracies)
print(f"Best k: {best_k}, with Accuracy: {best_accuracy:.4f}")